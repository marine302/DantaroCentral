"""
Background task for continuous market analysis.

This module implements scheduled tasks for:
- Fetching latest market data from exchanges
- Updating support levels and recommendations
- Cleaning old data and maintaining cache

Runs every 5 minutes as configured in settings.
"""
import asyncio
import logging
import structlog
from datetime import datetime, timedelta
from typing import Dict, Any
import json
from dataclasses import asdict

from app.core.config import settings
from app.services.market_data_service import MarketDataService
from app.services.real_market_service import RealMarketDataService
from app.services.cache_service import CacheService
from app.domain.recommenders.advanced_recommender import CoinRecommender
from app.domain.recommenders.volume_recommender import VolumeBasedRecommender
from app.domain.calculators.support_calculator import SupportLevelCalculator
from app.database.redis_cache import redis_manager

# ë¡œê±° ì„¤ì •
logger = structlog.get_logger(__name__)
logging.getLogger(__name__).setLevel(logging.INFO)

# ì„œë¹„ìŠ¤ ë¡œê±° ì„¤ì •
logging.getLogger('app.services.real_market_service').setLevel(logging.INFO)


class MarketAnalyzer:
    """
    Background task manager for continuous market analysis.
    
    This class handles periodic market data updates and analysis
    to keep the central server's recommendations fresh.
    """
    
    def __init__(self):
        self.market_service = MarketDataService()
        self.real_market_service = RealMarketDataService()
        self.cache_service = CacheService()
        # ê¸°ì¡´ ì¶”ì²œê¸° ë° ìƒˆë¡œìš´ ê±°ë˜ëŸ‰ ê¸°ë°˜ ì¶”ì²œê¸° ì´ˆê¸°í™”
        self.coin_recommender = CoinRecommender()
        self.volume_recommender = VolumeBasedRecommender()  # ê±°ë˜ëŸ‰ ê¸°ë°˜ ë‹¨íƒ€ ì¶”ì²œê¸°
        self.last_analysis: Dict[str, datetime] = {}
        self.is_running = False
        self.worker_id = "market_analyzer_main"
        self.start_time = datetime.utcnow()
    
    async def start_continuous_analysis(self):
        """Start the continuous analysis loop."""
        if self.is_running:
            logger.warning("Market analyzer already running")
            return
        
        logger.info("ï¿½ï¿½ï¿½ ì‹œì‘: ì‹¤ì‹œê°„ ì‹œì¥ ë¶„ì„ ë£¨í”„ ì‹œì‘")
        
        try:
            # í™•ì‹¤í•˜ê²Œ ì‹¤í–‰ ìƒíƒœ ì„¤ì •
            self.is_running = True
            
            # ì´ˆê¸° ìƒíƒœ ì—…ë°ì´íŠ¸ ë° í•˜íŠ¸ë¹„íŠ¸ ì„¤ì •
            self._update_worker_status()
            heartbeat_task = asyncio.create_task(self._heartbeat_loop())
            
            # Run first analysis cycle immediately
            logger.info("ğŸ”„ ì¦‰ì‹œ ì²« ë²ˆì§¸ ë¶„ì„ ì‚¬ì´í´ ì‹¤í–‰ ì‹œì‘")
            await self.run_analysis_cycle()
            logger.info("âœ… ì²« ë²ˆì§¸ ë¶„ì„ ì‚¬ì´í´ ì™„ë£Œ, ì£¼ê¸°ì  ë£¨í”„ ì‹œì‘")
            
            # ìƒíƒœ ì¬í™•ì¸
            if not self.is_running:
                logger.error("âŒ ì²« ë²ˆì§¸ ë¶„ì„ í›„ is_runningì´ Falseë¡œ ë³€ê²½ë¨. ë‹¤ì‹œ ì‹œì‘")
                self.is_running = True
                self._update_worker_status()
            
            # ì£¼ê¸°ì  ì‹¤í–‰ ë£¨í”„
            while self.is_running:
                logger.info(f"â° ë‹¤ìŒ ë¶„ì„ ì‚¬ì´í´ê¹Œì§€ ëŒ€ê¸° ì¤‘... ({settings.market_analysis_interval}ì´ˆ)")
                await asyncio.sleep(settings.market_analysis_interval)
                logger.info("ğŸ”„ ì˜ˆì•½ëœ ë¶„ì„ ì‚¬ì´í´ ì‹¤í–‰ ì¤‘")
                await self.run_analysis_cycle()
                
                # ë¶„ì„ í›„ ìƒíƒœ ì—…ë°ì´íŠ¸
                self._update_worker_status()
        except Exception as e:
            logger.error(f"âŒ ì‹œì¥ ë¶„ì„ ë£¨í”„ ì‹¤íŒ¨: {e}", exc_info=True)
            self.is_running = False
        finally:
            # í˜¹ì‹œë¼ë„ ë£¨í”„ê°€ ì¢…ë£Œë˜ë©´ ìƒíƒœ ì—…ë°ì´íŠ¸
            if self.is_running:
                logger.warning("âš ï¸ ì˜ˆìƒì¹˜ ì•Šê²Œ ë¶„ì„ ë£¨í”„ ì¢…ë£Œë¨")
                self.is_running = False
                self._update_worker_status()
    
    async def stop_analysis(self):
        """Stop the continuous analysis loop."""
        logger.info("Stopping market analysis")
        self.is_running = False
    
    async def run_analysis_cycle(self):
        """Run a single analysis cycle."""
        try:
            cycle_start = datetime.utcnow()
            logger.info("Starting market analysis cycle")
            
            # Step 1: Fetch latest market data
            logger.info("Step 1: Fetching market data")
            market_data = await self._fetch_market_data()
            
            if not market_data:
                logger.warning("No market data available for analysis")
                return
            
            # Step 2: Update recommendations
            logger.info("Step 2: Updating recommendations")
            await self._update_recommendations(market_data)
            
            # Step 3: Update support levels for top coins
            logger.info("Step 3: Updating support levels")
            await self._update_support_levels(market_data)
            
            # Step 4: Clean old cache entries
            logger.info("Step 4: Cleaning cache")
            await self._cleanup_cache()
            
            # Step 5: Log analysis completion
            cycle_duration = (datetime.utcnow() - cycle_start).total_seconds()
            logger.info(f"Analysis cycle completed in {cycle_duration:.2f}s")
            
            # Update last analysis timestamp
            self.last_analysis['full_cycle'] = cycle_start
            
        except Exception as e:
            logger.error(f"Analysis cycle failed: {e}", exc_info=True)
    
    async def _fetch_market_data(self) -> Dict[str, Dict]:
        """Fetch latest market data from all sources."""
        try:
            logger.debug("Fetching market data from exchanges")
            
            # Get real market data first
            market_data = await self.real_market_service.get_market_data()
            
            # Fallback to mock data if real data fails
            if not market_data:
                logger.warning("Real market data unavailable, using fallback service")
                market_data = await self.market_service.get_all_market_data()
            
            if market_data:
                logger.info(f"Fetched data for {len(market_data)} symbols")
            
            return market_data
            
        except Exception as e:
            logger.error(f"Failed to fetch market data: {e}")
            return {}
    
    async def _update_recommendations(self, market_data: Dict[str, Dict]):
        """Update coin recommendations based on fresh market data."""
        try:
            logger.debug("Updating coin recommendations")
            
            # ê±°ë˜ëŸ‰ ê¸°ë°˜ ë‹¨íƒ€ ì¶”ì²œ ìƒì„± (ê¸°ë³¸ ì‚¬ìš© ëª¨ë“œ)
            logger.info("Generating volume-based scalping recommendations")
            volume_recommendations = await self.volume_recommender.get_recommendations(
                market_data, 
                settings.top_recommendations_count
            )
            
            # ê¸°ì¡´ ë‹¤ì¤‘ ì „ëµ ì¶”ì²œë„ ìƒì„± (ë°±ì—… ë° ë¹„êµìš©)
            logger.info("Generating traditional multi-strategy recommendations")
            traditional_recommendations = await self.coin_recommender.get_recommendations(
                market_data, 
                settings.top_recommendations_count
            )
            
            # ê±°ë˜ëŸ‰ ê¸°ë°˜ ë‹¨íƒ€ ì¶”ì²œ ìºì‹œ
            if volume_recommendations:
                volume_cache_key = f"recommendations:volume:{settings.top_recommendations_count}"
                
                volume_response_data = {
                    'recommendations': volume_recommendations,
                    'total_analyzed': len(market_data),
                    'cache_timestamp': datetime.utcnow().isoformat(),
                    'metadata': {
                        'analysis_method': 'volume_based_scalping',
                        'criteria': ['volume', 'volatility', 'liquidity'],
                        'top_n': settings.top_recommendations_count,
                        'auto_generated': True
                    }
                }
                
                await self.cache_service.set(
                    volume_cache_key,
                    volume_response_data,
                    ttl=settings.strategy_cache_ttl
                )
                
                logger.info(f"Updated volume-based recommendations for {len(volume_recommendations)} coins")
                
                # ê¸°ë³¸ ì¶”ì²œ í‚¤ì—ë„ ê±°ë˜ëŸ‰ ê¸°ë°˜ ì¶”ì²œ ì €ì¥ (ê¸°ë³¸ ëª¨ë“œ)
                await self.cache_service.set(
                    f"recommendations:{settings.top_recommendations_count}",
                    volume_response_data,
                    ttl=settings.strategy_cache_ttl
                )
            
            # ê¸°ì¡´ ë‹¤ì¤‘ ì „ëµ ì¶”ì²œ ìºì‹œ (ë°±ì—…ìš©)
            if traditional_recommendations:
                traditional_cache_key = f"recommendations:traditional:{settings.top_recommendations_count}"
                
                traditional_response_data = {
                    'recommendations': traditional_recommendations,
                    'total_analyzed': len(market_data),
                    'cache_timestamp': datetime.utcnow().isoformat(),
                    'metadata': {
                        'analysis_methods': ['technical', 'volume', 'volatility'],
                        'top_n': settings.top_recommendations_count,
                        'auto_generated': True
                    }
                }
                
                await self.cache_service.set(
                    traditional_cache_key,
                    traditional_response_data,
                    ttl=settings.strategy_cache_ttl
                )
                
                logger.info(f"Cached traditional recommendations for {len(traditional_recommendations)} coins")
            
            # Redisì— ì¶”ì²œ ê²°ê³¼ ì €ì¥
            try:
                # CoinRecommendation ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                def convert_to_dict(obj) -> Dict[str, Any]:
                    if isinstance(obj, dict):
                        return obj
                    elif hasattr(obj, '__dict__'):
                        return dict(obj.__dict__)
                    else:
                        return {"data": str(obj)}
                
                volume_dicts = [convert_to_dict(rec) for rec in volume_recommendations]
                traditional_dicts = [convert_to_dict(rec) for rec in traditional_recommendations]
                
                # ë™ê¸° í˜¸ì¶œë¡œ ë³€ê²½
                result = redis_manager.save_recommendations_to_redis(volume_dicts, traditional_dicts)
                logger.debug(f"Redis save result: {result}")
            except Exception as e:
                logger.error(f"Failed to save recommendations to Redis: {e}")
            
            # WebSocketì„ í†µí•œ ë¸Œë¡œë“œìºìŠ¤íŠ¸ (ëª¨ë“  í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ìµœì‹  ì¶”ì²œ ì „ì†¡)
            try:
                # CoinRecommendation ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                def convert_to_dict(obj) -> Dict[str, Any]:
                    if isinstance(obj, dict):
                        return obj
                    elif hasattr(obj, '__dict__'):
                        return dict(obj.__dict__)
                    else:
                        return {"data": str(obj)}
                        
                volume_dicts = [convert_to_dict(rec) for rec in volume_recommendations]
                # ë™ê¸° í˜¸ì¶œë¡œ ë³€ê²½
                result = redis_manager.broadcast_recommendations(volume_dicts)
                logger.debug(f"Broadcast result: {result}")
            except Exception as e:
                logger.error(f"Failed to broadcast recommendations via WebSocket: {e}")
            
        except Exception as e:
            logger.error(f"Failed to update recommendations: {e}", exc_info=True)
    
    async def _update_support_levels(self, market_data: Dict[str, Dict]):
        """Update support levels for top performing coins."""
        try:
            logger.debug("Updating support levels")
            
            # Get top coins (first 20 from recommendations)
            top_symbols = list(market_data.keys())[:20]
            
            updated_count = 0
            
            for symbol in top_symbols:
                try:
                    # Get price history
                    price_history = await self.market_service.get_price_history(
                        symbol, 
                        settings.support_level_lookback_days
                    )
                    
                    if not price_history:
                        continue
                    
                    # Calculate support levels
                    support_levels = SupportLevelCalculator.calculate_support_levels(price_history)
                    
                    if support_levels:
                        # Format and cache the results
                        response_data = {
                            'symbol': symbol,
                            'support_levels': {},
                            'calculation_timestamp': datetime.utcnow().isoformat(),
                            'metadata': {
                                'price_data_points': len(price_history),
                                'lookback_days': settings.support_level_lookback_days,
                                'auto_generated': True
                            }
                        }
                        
                        # Convert support levels to response format
                        for level_type, level_data in support_levels.items():
                            response_data['support_levels'][level_type] = {
                                'price': float(level_data.price),
                                'confidence': level_data.confidence,
                                'calculation_method': level_data.calculation_method,
                                'lookback_days': level_data.lookback_days,
                                'metadata': level_data.metadata
                            }
                        
                        # Cache the support levels
                        cache_key = f"support_levels:{symbol}"
                        await self.cache_service.set(
                            cache_key,
                            response_data,
                            ttl=settings.strategy_cache_ttl
                        )
                        
                        updated_count += 1
                
                except Exception as e:
                    logger.warning(f"Failed to update support levels for {symbol}: {e}")
                    continue
            
            logger.info(f"Updated support levels for {updated_count} symbols")
            
        except Exception as e:
            logger.error(f"Failed to update support levels: {e}")
    
    async def _cleanup_cache(self):
        """Clean up old cache entries and expired data."""
        try:
            logger.debug("Cleaning up cache")
            
            # Get cache statistics
            stats = await self.cache_service.get_stats()
            
            if stats.get('expired_keys', 0) > 0:
                logger.info(f"Cache cleanup: {stats['expired_keys']} expired entries found")
            
            # Note: The cache service automatically removes expired entries
            # when they are accessed, so no manual cleanup is needed for the
            # in-memory implementation. In a Redis implementation, you would
            # run SCAN and DELETE commands here.
            
        except Exception as e:
            logger.error(f"Cache cleanup failed: {e}")
    
    def _update_worker_status(self):
        """ì›Œì»¤ ìƒíƒœ ì—…ë°ì´íŠ¸ ë° Redisì— í•˜íŠ¸ë¹„íŠ¸ ê¸°ë¡"""
        try:
            worker_status = {
                'worker_id': self.worker_id,
                'last_heartbeat': datetime.utcnow().isoformat(),
                'is_running': self.is_running,
                'uptime_seconds': (datetime.utcnow() - self.start_time).total_seconds(),
                'stats': {
                    'last_analysis_time': self.last_analysis.get('full_cycle', datetime.utcnow()).isoformat() if self.last_analysis.get('full_cycle') else None,
                    'analysis_count': len(self.last_analysis),
                    'volume_recommender': self.volume_recommender.get_stats() if hasattr(self.volume_recommender, 'get_stats') else {},
                }
            }
            
            # Redisì— ìƒíƒœ ì €ì¥
            redis_manager.set_worker_status(self.worker_id, worker_status)
            logger.debug(f"Worker status updated: {self.worker_id} is {'running' if self.is_running else 'stopped'}")
            
        except Exception as e:
            logger.error(f"Failed to update worker status: {e}")
    
    async def _send_heartbeat(self):
        """ì£¼ê¸°ì ìœ¼ë¡œ ì›Œì»¤ í•˜íŠ¸ë¹„íŠ¸ ì „ì†¡"""
        try:
            self._update_worker_status()
        except Exception as e:
            logger.error(f"Failed to send heartbeat: {e}")
    
    async def _heartbeat_loop(self):
        """ì£¼ê¸°ì ìœ¼ë¡œ í•˜íŠ¸ë¹„íŠ¸ ì „ì†¡"""
        while self.is_running:
            try:
                await self._send_heartbeat()
                await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ í•˜íŠ¸ë¹„íŠ¸ ì „ì†¡
            except Exception as e:
                logger.error(f"Heartbeat loop error: {e}")
                await asyncio.sleep(60)  # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê³„ì† ì‹œë„


# Global market analyzer instance
market_analyzer = MarketAnalyzer()


async def start_background_tasks():
    """Start all background tasks."""
    logger.info("ğŸš€ğŸš€ğŸš€ ì‹œì‘: ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘")
    
    try:
        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ìƒíƒœ í™•ì¸ ë° ì¬ì„¤ì •
        if market_analyzer.is_running:
            logger.warning("âš ï¸ Market analyzer was already running. Restarting...")
            await market_analyzer.stop_analysis()
            await asyncio.sleep(1)  # ìƒíƒœ ì´ˆê¸°í™” ëŒ€ê¸°
        
        # ë¡œê¹… ë ˆë²¨ ìƒí–¥ ì¡°ì • (ë””ë²„ê¹…)
        logging.getLogger('app.tasks.market_analyzer').setLevel(logging.DEBUG)
        logging.getLogger('app.services.real_market_service').setLevel(logging.DEBUG)
        
        # Start market analysis in background
        logger.info("ğŸ”„ Starting continuous analysis task...")
        task = asyncio.create_task(market_analyzer.start_continuous_analysis())
        
        # Wait longer to ensure the task really starts
        await asyncio.sleep(3)
        
        logger.info(f"âœ“ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ìƒíƒœ: {market_analyzer.is_running}")
        
        # ë””ë²„ê¹… ì •ë³´ ê¸°ë¡
        if market_analyzer.is_running:
            logger.info("âœ…âœ…âœ… ì™„ë£Œ: market_analyzer.start_continuous_analysis() ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë¨")
        else:
            logger.error("âŒâŒâŒ ì˜¤ë¥˜: market_analyzer.start_continuous_analysis() ì‘ì—…ì´ ìƒì„±ë˜ì—ˆì§€ë§Œ is_runningì´ Falseì„")
            # ê°•ì œë¡œ ì²« ë¶„ì„ ì‹¤í–‰ ì‹œë„
            logger.info("ğŸ’¢ ìˆ˜ë™ìœ¼ë¡œ ì²« ë²ˆì§¸ ë¶„ì„ ì‚¬ì´í´ ì‹¤í–‰ ì‹œë„...")
            await market_analyzer.run_analysis_cycle()
        
        return task
    except Exception as e:
        logger.error(f"âŒâŒâŒ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘ ì‹¤íŒ¨: {e}", exc_info=True)
        raise


async def stop_background_tasks():
    """Stop all background tasks."""
    logger.info("Stopping background tasks")
    await market_analyzer.stop_analysis()
